{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14393716,"sourceType":"datasetVersion","datasetId":9192631}],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"API_KEY\")\nsecret_value_1 = user_secrets.get_secret(\"GROQ_API_KEY\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nPDF_PATH = \"/kaggle/input/one-nation-one-election/One_Nation_One_Election_Explained.pdf\"\n\nprint(\"Uploaded PDF:\", PDF_PATH)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install numpy pillow langchain groq\n!pip install langchain-groq langchain-community langchain-text-splitters chromadb pypdf\n!pip install -q gradio\n!pip install sentence-transformers\n!pip install moviepy diffusers transformers accelerate\n!apt-get update && apt-get install -y ffmpeg\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_classic.chains import RetrievalQA\nimport numpy as np\nfrom PIL import Image\nimport torch\n\nOUTPUT_DIR = \"output\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nvideo_paths = []\n\nos.environ[\"GROQ_API_KEY\"] = secret_value_1\n# Load & Split PDF\nPDF_PATH = \"/kaggle/input/one-nation-one-election/One_Nation_One_Election_Explained.pdf\"\nloader = PyPDFLoader(PDF_PATH)\ndocuments = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\nchunks = text_splitter.split_documents(documents)\n\n# Embeddings & Vector DB\nembeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\nvector_db = Chroma.from_documents(documents=chunks, embedding=embeddings)\n\n# LLM & RAG\n\nllm = ChatGroq(model_name=\"groq/compound\")\nrag = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vector_db.as_retriever())\n\n# Summary\nsummary_prompt = \"Create a short, engaging summary for a video.\\nRules:\\n- Simple language\\n- Educational tone\\n- 5‚Äì6 key ideas\"\nsummary = rag.invoke(summary_prompt)[\"result\"]\nprint(\"\\nSUMMARY:\\n\", summary)\n\n# Scene Script\nscene_prompt = f\"\"\"\n\nFor each scene, STRICTLY use this format:\n\"Scene [Number]: [Subject], [Action/Pose], [Setting/Background], [Lighting/Atmosphere], [Camera Angle], [Art Style/Quality Keywords]\"\n\nYou are a visual storyboard generator for educational videos.\n\nYour task is to convert a given summary into EXACTLY 5 image prompts\nfor a kid-friendly educational animation.\n\nCRITICAL RULES (must follow strictly):\n\n1. Each image must express ONLY ONE simple visual idea.\n2. Use ONLY concrete, physical objects and environments.\n3. Avoid all abstract concepts, symbols, icons, metaphors, or text.\n4. Do NOT use flags, maps, question marks, signs, numbers, letters, or written words.\n5. Do NOT describe political parties, leaders, slogans, or propaganda.\n6. Human faces are NOT allowed.\n   - If humans appear, show only distant figures, silhouettes from behind, or hands.\n7. Each prompt must be visually simple and uncluttered.\n8. Do NOT combine multiple concepts in a single scene.\n9. Let meaning emerge from the SEQUENCE of images, not from one image.\n10. Use calm, neutral, non-threatening visuals suitable for children.\n\nSTYLE CONSTRAINTS:\n\n- Kid comic book illustration style\n- Simple shapes and clean layouts\n- Soft, natural lighting\n- Bright but gentle colors\n- Wide or medium shots only\n- No dramatic or cinematic exaggeration\n- No photorealistic or hyper-detailed skin\n- No uncanny realism\n\nEach image prompt MUST include:\n- Subject (main object or environment)\n- Setting (simple background)\n- Lighting (soft, natural)\n- Camera distance (wide or medium)\n- Style: \"kid comic book illustration style\"\n\nDO NOT:\n- Explain the scene\n- Add titles or captions\n- Add meanings or interpretations\n- Mention the summary or policy name\n\n\nSUMMARY TO CONVERT INTO SCENES:\n{summary}\n\nReturn EXACTLY 5 scenes in this format:\n\nScene 1: visual description\nScene 2: visual description\nScene 3: visual description\nScene 4: visual description\nScene 5: visual description\n\n\n\"\"\"\nscene_script = llm.invoke(scene_prompt).content\nprint(\"\\nSCENES:\\n\", scene_script)\n\nscenes = [line.split(\":\", 1)[1].strip() for line in scene_script.split(\"\\n\") if line.lower().startswith(\"scene\")]\nprint(len(scenes))\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\n\n\nAPI_KEY = secret_value_0  # replace with your key\nVOICE_ID = \"MbmiTDI5YuxpN7UUP2St\"    # default ElevenLabs voice; you can change\nOUTPUT_FILE = \"speech.wav\"\n\n\naudio_script=f\"\"\"convert the given {summary} into a narrative storytelling within 100 words in hindi language\"\"\"\nprint(audio_script)\naudio_prompt=llm.invoke(audio_script).content\n\nurl = f\"https://api.elevenlabs.io/v1/text-to-speech/{VOICE_ID}\"\n\nheaders = {\n    \"xi-api-key\": API_KEY,\n    \"Content-Type\": \"application/json\"\n}\n\npayload = {\n    \"text\":audio_prompt,\n    \"voice_settings\": {\n        \"stability\": 0.7,   # optional: 0-1\n        \"similarity_boost\": 0.7\n    }\n}\n\nresponse = requests.post(url, json=payload, headers=headers)\n\nif response.status_code == 200:\n    with open(OUTPUT_FILE, \"wb\") as f:\n        f.write(response.content)\n    print(f\"‚úÖ Audio saved as {OUTPUT_FILE}\")\nelse:\n    print(\"‚ùå Failed to generate audio\")\n    print(response.status_code, response.text)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom diffusers import StableDiffusionXLPipeline\nfrom moviepy.editor import (\n    ImageClip,\n    concatenate_videoclips,\n    AudioFileClip\n)\nimport os\n\n# --- Configuration ---\nOUTPUT_DIR = \"election_video\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n\nprint(\"üöÄ Loading....\")\npipe = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16,\n    use_safetensors=True\n).to(\"cuda\")\n\n\n# --- Generate Images ---\nimage_files = []\n\nfor i, prompt in enumerate(scenes, start=1):\n    print(f\"üé® Generating {i}/{len(scenes)}...\")\n    image = pipe(\n        prompt,\n        num_inference_steps = 30,     # Good tradeoff: quality vs speed\n        guidance_scale = 6.0,         # Strong adherence to prompt, but not over-detailed\n        width = 768,                 # Use 512 instead of 768 for free Colab GPU\n        height = 768,                 # Keeps VRAM usage low\n        ).images[0]\n\n    path = f\"{OUTPUT_DIR}/scene_{i}.png\"\n    image.save(path)\n    image_files.append(path)\n\n# --- Load Audio ---\naudio = AudioFileClip(\"speech.wav\")\naudio_duration = audio.duration\n\n# --- Create Video Clips (synced to audio) ---\nper_image_duration = audio_duration / len(image_files)\n\nclips = [\n    ImageClip(img).set_duration(per_image_duration)\n    for img in image_files\n]\n\nfinal_video = concatenate_videoclips(clips, method=\"compose\")\nfinal_video = final_video.set_audio(audio)\n\n# --- Export ---\nfinal_video.write_videofile(\n    f\"{OUTPUT_DIR}/election_final.mp4\",\n    fps=24,\n    codec=\"libx264\",\n    audio_codec=\"aac\"\n)\n\nprint(\"\\nüéâ SUCCESS: Your photorealistic video with synced audio is ready!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"## üé¨ Generated Video\")\n    gr.Video(value=\"election_video/election_final.mp4\")\n\n    gr.Markdown(\"## üìù Summary\")\n    gr.Textbox(value=summary, lines=6)\n\ndemo.launch(share=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}